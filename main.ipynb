{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyCNywAuEMOiGUkRNQa8u5UyFKPq1As8sBQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install youtube_channel_transcript_api\n",
    "%pip install --upgrade google-api-python-client\n",
    "%pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "%pip install elasticsearch\n",
    "%pip install sentence_transformers\n",
    "%pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_channel_transcript_api import *\n",
    "from elasticsearch import Elasticsearch\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLIST_ID = \"PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\" \n",
    "channel_getter = YoutubePlaylistTranscripts(\"Some Gibberish Name\",PLAYLIST_ID, API_KEY) #channel getter is a YoutubePlaylistTranscripts Object\n",
    "# channel_getter is an object of 'YoutubePlaylistTranscripts' Type\n",
    "\n",
    "# for index, item in enumerate(channel_getter.video):\n",
    "#     print(f\"{index+1}. Video: {item[0]}, ID: {item[1]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching videos data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded: 21\n",
      "Number of videos data fetched: 21\n",
      "Number of videos data errored: 0\n"
     ]
    }
   ],
   "source": [
    "videos_data, videos_errored = channel_getter.get_transcripts(languages=['en'])\n",
    "\n",
    "print(f'Number of videos loaded: {len(channel_getter.video)}')\n",
    "print(f'Number of videos data fetched: {len(videos_data)}')\n",
    "print(f'Number of videos data errored: {len(videos_errored)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a backup on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_data, f)\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_errored.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_errored, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get a list of videos loaded from the playlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded from playlist: 21\n",
      "List of loaded videos:\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of loaded videos:')\n",
    "\n",
    "# for index, item in enumerate(videos_data):\n",
    "#     print(f'{index+1}. Video ID: {item}        Title:', videos_data[item]['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded from playlist: 21\n",
      "List of non-loaded videos:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of non-loaded videos:')\n",
    "print(videos_errored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating caption dataset on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"./content/playlists\"\n",
    "CHANNEL_DIRECTOR_NAME = PLAYLIST_ID\n",
    "\n",
    "SAVE_FOLDER = os.path.join(ROOT_FOLDER, CHANNEL_DIRECTOR_NAME)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid_obj in videos_data.values():\n",
    "  TITLE = vid_obj['title']\n",
    "  #windows doesn't allow all the special characters to be there in the folder name\n",
    "  # Let's remove the special characters from the title\n",
    "\n",
    "  TITLE = TITLE.replace(\"?\",'')   #windows doesn't support '?'\n",
    "  TITLE = TITLE.replace(\"|\",'')   #windows doesn't support '|'\n",
    "\n",
    "  VID_FOLDER = os.path.join(SAVE_FOLDER, TITLE)\n",
    "  # print(f'VID_FOLDER: {VID_FOLDER}')\n",
    "  vid_exists = os.path.exists(VID_FOLDER)   # checking whether the video directory exists\n",
    "  # print(f'vid_exists: {vid_exists}')\n",
    "  os.makedirs(VID_FOLDER) if not vid_exists else None   # if the directory doesn't exist, create one\n",
    "\n",
    "  vid_captions = vid_obj['captions'] \n",
    "\n",
    "  full_vid_captions = [f'Title: {TITLE}']  #This list will have all the captions in the video without the time stamps\n",
    "  #The below code can be modified to include time\n",
    "  for caption in vid_captions:\n",
    "    full_vid_captions.append(caption['text'])   #full video captions is the list of caption strings\n",
    "\n",
    "  full_vid_captions = \" \".join(full_vid_captions)   # this returns a single string of complete video caption\n",
    "\n",
    "  with open(os.path.join(VID_FOLDER, f'{TITLE}_captions.txt'), 'w') as f:\n",
    "    f.write(full_vid_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(text:str) -> str:\n",
    "    # this function tries to clean the text by removing multiple new lines, adding paragraph breaks, and removing empty paragraphs\n",
    "\n",
    "    # getting rid of all new lines\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', '')\n",
    "\n",
    "    # will add some features here in future\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Document Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import hashlib\n",
    "import mmh3\n",
    "from typing import List, Dict, Optional, Generator, Set, Union\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, meta, hash_id, title:str, content:str, language:str = 'English', score:float = None, hash_id_keys:List[str] = None):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.language = language\n",
    "        self.hash_id_keys = hash_id_keys\n",
    "        self.meta = meta or {}\n",
    "        self.embedding = None\n",
    "        self.score = score\n",
    "\n",
    "        if hash_id is None: \n",
    "            self.hash_id = self.generate_hash(hash_id_keys)\n",
    "        else:\n",
    "            self.hash_id = hash_id\n",
    "\n",
    "    def generate_hash(self, hash_id_keys):\n",
    "        return \"{:02x}\".format(mmh3.hash128(str(self.content), signed=False))\n",
    "\n",
    "    def to_dict(self, field_map = {}):\n",
    "        inv_field_map = {v:k for k, v in field_map.items()}\n",
    "        _doc: Dict[str, str] = {}\n",
    "\n",
    "        for k, v in self.__dict__.items():\n",
    "            # Exclude other fields (Pydantic, ..) fields from the conversion process\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            k = k if k not in inv_field_map else inv_field_map[k]\n",
    "            _doc[k] = v\n",
    "        # print(f'_doc in to_dict is {_doc}')\n",
    "        return _doc\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Title: {self.title}\\nContent: {self.content}\\nLanguage: {self.language}\\nHash ID: {self.hash_id} \\nMetadata: {self.meta}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(document:Document, split_length:int = 100):\n",
    "    text = document.content\n",
    "\n",
    "    line = ''\n",
    "    text_chunks = []\n",
    "\n",
    "    words = text.split(' ')[:-1]\n",
    "\n",
    "    # print(words)\n",
    "\n",
    "    for word in words:\n",
    "        if len(line) >= split_length:\n",
    "            text_chunks.append(line)\n",
    "            line = ''\n",
    "\n",
    "        else:\n",
    "            line += ' ' + word\n",
    "            \n",
    "    # for sentence in (s.strip() + '.' for s in text.split('.')[:-1]):   \n",
    "    #     if len(line.split()) + len(sentence.split()) + 1 >= split_length:   # can't fit on that line => start a new one\n",
    "    #         text_chunks.append(line)\n",
    "    #         line = sentence\n",
    "            \n",
    "    #     else:       # can fit it => add a space and then the sentence\n",
    "    #         line += '' + sentence\n",
    "\n",
    "    # print(f'text chnks are: {text_chunks}')\n",
    "\n",
    "    documents = []\n",
    "    for i, txt in enumerate(text_chunks):\n",
    "        doc = Document(title = document.title, content = txt, hash_id = None, hash_id_keys=None, meta = {'filename': document.meta.copy()} or {})\n",
    "        # I need to implement meta data here\n",
    "        doc.meta[\"_split_id\"] = i\n",
    "        doc.meta[\"_parent_hash\"] = document.hash_id\n",
    "        documents.append(doc)\n",
    "        \n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:The file ./content/playlists\\PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\\The 3 MOST IMPORTANT JAZZ SCALES and how similar they are\\The 3 MOST IMPORTANT JAZZ SCALES and how similar they are_captions.txt cannot be opened.\n"
     ]
    }
   ],
   "source": [
    "next_folder = os.path.join(SAVE_FOLDER, os.listdir(SAVE_FOLDER)[0])\n",
    "file_path = os.path.join(next_folder, f'{os.listdir(SAVE_FOLDER)[0]}_captions.txt')\n",
    "\n",
    "parent_document = {}    # storing document objects with the hashid:object \n",
    "document_list = []      # this list stores all the document objects\n",
    "split = True\n",
    "\n",
    "# crawler\n",
    "for folder in os.listdir(SAVE_FOLDER):\n",
    "    # opening the files\n",
    "    next_folder = os.path.join(SAVE_FOLDER, folder)\n",
    "    file_path = os.path.join(next_folder, f'{folder}_captions.txt')\n",
    "    \n",
    "\n",
    "    try:\n",
    "        f = open(file_path, 'r')\n",
    "    except:\n",
    "        logging.error(f\"The file {file_path} cannot be opened.\")\n",
    "    \n",
    "    # creating document object \n",
    "    content = f.read()\n",
    "    obj = Document(title = folder, content = content, meta = {'file_name': f'{folder}_captions.txt'} , hash_id = None, hash_id_keys = None)\n",
    "\n",
    "    # cleaning the object content\n",
    "    obj.content = clean_document(obj.content)\n",
    "\n",
    "    # storing the content in the dictionary\n",
    "    parent_document[obj.hash_id] = obj\n",
    "\n",
    "\n",
    "    # if split is needed, we split else we directly append to the list\n",
    "    if split:\n",
    "        # split_document returns a list of document objects\n",
    "        documents = split_documents(obj, split_length = 1000)\n",
    "\n",
    "\n",
    "        # appending the list of document objects to our main list\n",
    "        for d in documents:\n",
    "            document_list.append(d)\n",
    "        \n",
    "    else:\n",
    "        document_list.append(obj)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ES!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"D:\\BTP\\youtubeQandA\\http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", 'vWr8xqxdlmOhj*Q_2yFI')\n",
    ")\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to ES!\")\n",
    "else:\n",
    "    print(\"Could not connect!\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "\n",
    "def bulk_index_documents(documents_to_index, request_timeout = 300, refresh = 'wait_for'):\n",
    "    try:\n",
    "        bulk(es, documents_to_index, request_timeout = request_timeout, refresh = refresh)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to index batch of {len(documents_to_index)} documents because of too many request response\")\n",
    "\n",
    "\n",
    "def create_mappings_document(\n",
    "    index_name = 'document',\n",
    "    analyzer = 'standard',\n",
    "    custom_mapping = None,\n",
    "    name_field = \"title\",\n",
    "    content_field = \"content\",\n",
    "    embedding_field = 'embedding',\n",
    "    embedding_dim = 768,\n",
    "    search_fields = ['content'],\n",
    "    synonyms = None,\n",
    "    synonym_type = 'synonym'):\n",
    "    \n",
    "\n",
    "    if custom_mapping:\n",
    "        mapping = custom_mapping\n",
    "    else:\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {name_field : {\"type\" : \"keyword\"}, content_field: {\"type\": \"text\"}},\n",
    "                \"dynamic_templates\": [\n",
    "                    {\"strings\": {\"path_match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": {\"type\": \"keyword\"}}}\n",
    "                ],\n",
    "            },\n",
    "            \"settings\": {\"analysis\": {\"analyzer\": {\"default\": {\"type\": analyzer}}}},\n",
    "        }\n",
    "\n",
    "    if synonyms:\n",
    "        for field in search_fields:\n",
    "            mapping[\"mappings\"][\"properties\"].update({field: {\"type\": \"text\", \"analyzer\": \"synonym\"}})\n",
    "        mapping[\"mappings\"][\"properties\"][content_field] = {\"type\": \"text\", \"analyzer\": \"synonym\"}\n",
    "\n",
    "        mapping[\"settings\"][\"analysis\"][\"analyzer\"][\"synonym\"] = {\n",
    "            \"tokenizer\": \"whitespace\",\n",
    "            \"filter\": [\"lowercase\", \"synonym\"],\n",
    "        }\n",
    "\n",
    "        mapping[\"settings\"][\"analysis\"][\"filter\"] = {\n",
    "            \"synonym\": {\"type\": synonym_type, \"synonyms\": synonyms}\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        for field in search_fields:\n",
    "            mapping[\"mappings\"][\"properties\"].update({field: {\"type\": \"text\"}})\n",
    "\n",
    "    if embedding_field:\n",
    "        mapping[\"mappings\"][\"properties\"][embedding_field] = {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": embedding_dim\n",
    "        }\n",
    "\n",
    "    es.indices.create(index = index_name, ignore = 400, body = mapping)\n",
    "\n",
    "\n",
    "def index_documents(documents, index = 'document', batch_size = 100, refresh_type = 'wait_for'):\n",
    "    if index and not es.indices.exists(index= index):\n",
    "        logging.info('Creating mappings for the index as user did not provide any custom mapping...')\n",
    "        create_mappings_document(index_name = index)\n",
    "\n",
    "    else:\n",
    "        logging.info('Using custom mapping...')\n",
    "\n",
    "    documents_to_index = []\n",
    "\n",
    "    # Iterating through all the documents and indexing them together\n",
    "    for i, doc in enumerate(documents):\n",
    "        \n",
    "        \n",
    "        # First we convert the document object into dict to follow ES conventions\n",
    "        _doc = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index,\n",
    "            **doc.to_dict()\n",
    "        }\n",
    "        # print(_doc)\n",
    "\n",
    "        _doc[\"_id\"] = str(i)\n",
    "\n",
    "        # Cast the embedding type as ES does not support numpy\n",
    "        if _doc['embedding'] is not None:\n",
    "            if (type(_doc['embedding']) == np.ndarray):\n",
    "                _doc['embedding'] = _doc['embedding'].tolist()\n",
    "\n",
    "        # don't index query score and empty fields\n",
    "        _ = _doc.pop(\"score\", None)\n",
    "        _doc = {k: v for k, v in _doc.items() if v is not None}\n",
    "\n",
    "        # For flat structure generally used in Elastic Search\n",
    "        # we 'unnest' all value within \"meta\"\n",
    "        if \"meta\" in _doc.keys():\n",
    "            for k, v in _doc[\"meta\"].items():\n",
    "                _doc[k] = v\n",
    "            _doc.pop(\"meta\")\n",
    "\n",
    "        documents_to_index.append(_doc)\n",
    "\n",
    "        if len(documents_to_index) % batch_size == 0:\n",
    "            logging.info(f'Indexing {len(documents_to_index)} documents')\n",
    "            bulk_index_documents(documents_to_index, request_timeout=300, refresh = refresh_type)\n",
    "            documents_to_index = []\n",
    "\n",
    "        if documents_to_index:\n",
    "            logging.info(f'Indexing {len(documents_to_index)} documents')\n",
    "            bulk_index_documents(documents_to_index, request_timeout=300, refresh = refresh_type)\n",
    "\n",
    "def clear_indices(index_name = ''):\n",
    "    \n",
    "    if index_name == '':\n",
    "        indices = list(es.indices.get_alias(\"*\").keys())\n",
    "    else:\n",
    "        indices = list(es.indices.get_alias(index_name).keys())\n",
    "\n",
    "    if indices:\n",
    "        logging.info(f'Wiping out all the documents belonging to the following indices: {indices}')\n",
    "\n",
    "        es.delete_by_query(index = indices, body = {\"query\": {\"match_all\": {}}})\n",
    "\n",
    "        logging.info(f\"Deleting Indices\")\n",
    "        for index in indices:\n",
    "            es.indices.delete(index= index, ignore= [400, 404])\n",
    "\n",
    "        print(list(es.indices.get_alias(\"*\".keys())))\n",
    "\n",
    "    else:\n",
    "        logging.info(\"No Indices are present in storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the query for the default BM25 retriever\n",
    "\n",
    "def construct_query(query, top_k, filters = None, all_terms_must_match = False):\n",
    "\n",
    "    # We choose if all terms must match or not (this can be used to enforce more strict rules)\n",
    "    operator = \"AND\" if all_terms_must_match else \"OR\"\n",
    "\n",
    "# There are multiple options for keyword based serach such as:\n",
    "# -> match: It directly matches all the keywords in any order\n",
    "# -> match_phrase: It directly matches all the keywords in specific order( so that sentences are bound to make sense)\n",
    "# -> multi_match: It directly matches all the keywords in any order but can match on multiple fields\n",
    "\n",
    "    body = {\n",
    "        \"size\": str(top_k),\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"content\", \"title\"],\n",
    "                            \"operator\": operator,\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "# This constructs query for dense retrieval using various scores\n",
    "def construct_query_dense(query_vector, top_k, similarity = \"cosinse\"):\n",
    "\n",
    "    if similarity == \"cosine\":\n",
    "        similarity_fn_name = \"cosineSimilarity\"\n",
    "    elif similarity == \"dot_product\":\n",
    "        similarity_fn_name = \"dotProduct\"\n",
    "    elif similarity == \"12\":\n",
    "        similarity_fn_name = \"12norm\"\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Invalid value for similarity in ElasticSearchDocumentStore\\nChoose between 'cosine', 'dot_product', and '12\"\n",
    "        )\n",
    "    \n",
    "    logging.info(f'Using the following similarity metric : {similarity}')\n",
    "\n",
    "    if (type(query_vector) == np.ndarray):\n",
    "        query_vector = query_vector.tolist()\n",
    "    \n",
    "    logging.info(f'The type of query vector is: {type(query_vector)}')\n",
    "\n",
    "    body = {\n",
    "        \"size\": str(top_k),\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": f\"{similarity_fn_name}(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vector}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "# This function processes the hit results obtained after elastic search\n",
    "def convert_es_dict(es_dict, return_embedding = False, scale_score = None):\n",
    "\n",
    "    meta_data = {k : v for k,v in es_dict['_source'].items() if k not in ('title', 'content', 'language', 'hash_id', 'embedding')}\n",
    "\n",
    "    # calculate score if using embedding retreival\n",
    "    score = es_dict['_score']\n",
    "\n",
    "    # check if name field is present or not\n",
    "    if es_dict['_source']['title'] is not None:\n",
    "        title = es_dict['_source']['title']\n",
    "\n",
    "    document = Document(title = title, content = es_dict['_source']['content'], language = es_dict['_source']['language'], meta = meta_data, score = score, hash_id = es_dict['_source']['hash_id'])\n",
    "    return document\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dense Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './data/tutorial'\n",
    "documents_dense, parent_dense = document_list, parent_document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# good for passage search\n",
    "sBERT = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBERT.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = sBERT.encode([doc.content for doc in documents_dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents_dense):\n",
    "    doc.embedding = encoded_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chambal Ka Daku\\AppData\\Local\\Temp\\ipykernel_5260\\979149385.py:7: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  bulk(es, documents_to_index, request_timeout = request_timeout, refresh = refresh)\n"
     ]
    }
   ],
   "source": [
    "index_documents(documents_dense, index = 'document_dense', batch_size = 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to define a reader for better results\n",
    "\n",
    "from transformers import BertForQuestionAnswering, AutoTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "modelname = 'deepset/roberta-base-squad2'\n",
    "\n",
    "reader = RobertaForQuestionAnswering.from_pretrained(modelname)\n",
    "reader_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline('question-answering', model = reader, tokenizer = reader_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import time\n",
    "import os\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_info(doc):\n",
    "    meta_dict = dict()\n",
    "    meta_dict['score'] = doc.score\n",
    "    meta_dict['title'] = doc.title\n",
    "    meta_dict['content'] = doc.content\n",
    "    meta_dict['meta'] = doc.meta\n",
    "    meta_dict['hash_id'] = doc.hash_id\n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    output = nlp({\n",
    "        'question' : question,\n",
    "        'context' : context,\n",
    "    })\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_documents(query, results):\n",
    "    answers = []\n",
    "\n",
    "    for res in results:\n",
    "        output = generate_answer(query, res['content'])\n",
    "        answers.append((output, res))\n",
    "\n",
    "    # Sort the answers according to the scores\n",
    "    sorted_answers = sorted(answers, key=lambda item: item[1].get('score'), reverse=True)\n",
    "    return sorted_answers\n",
    "\n",
    "def search(query, top_k, index, model, extract_answers:bool = True, all_terms_must_match:bool = True, combine:bool = True):\n",
    "\n",
    "    ### BM25 serach (lexical search) ###\n",
    "    t = time.time()\n",
    "\n",
    "    body = construct_query(query, top_k = top_k, all_terms_must_match= all_terms_must_match)\n",
    "    # print(f'body is {body}')\n",
    "\n",
    "    result = es.search(body = body, index = index)[\"hits\"][\"hits\"]\n",
    "    # print(f'results is {result}')\n",
    "    documents_lexical = [\n",
    "        convert_es_dict(hit, scale_score= None) for hit in result\n",
    "    ]\n",
    "    # print(documents_lexical[0])\n",
    "    documents_lexical = [fetch_info(doc) for doc in documents_lexical]\n",
    "    documents_lexical = process_documents(query, documents_lexical)\n",
    "\n",
    "    print('Top-3 lexical search (BM25) hits ')\n",
    "\n",
    "    for document in documents_lexical[0:3]:\n",
    "        print(\"\\n###################################\")\n",
    "\n",
    "        if extract_answers == True:\n",
    "            answers, result = document\n",
    "            answer = answers['answer']\n",
    "            print('Answer : ', answer)\n",
    "\n",
    "        else:\n",
    "            result = document\n",
    "\n",
    "    print('\\nRetrieval Score : ', result['score'])\n",
    "    print('Wiki Title : ', result['title'])\n",
    "    print(result['content'])\n",
    "    print('\\nOriginal Document meta data : ', result['meta'])\n",
    "\n",
    "    print('#######################')\n",
    "    print()\n",
    "\n",
    "    print('BM25 Results took a total of : {} seconds.'.format(time.time()-t))\n",
    "\n",
    "    #### SBERT Search (Semantic Search) #######\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    query_vector = sBERT.encode([query])[0].tolist()\n",
    "    body = construct_query_dense(query_vector, top_k= top_k, similarity = 'cosine')\n",
    "\n",
    "    result = es.search(body = body, index = index)['hits']['hits']\n",
    "\n",
    "    documents_semantic = [\n",
    "        convert_es_dict(hit, scale_score = None)\n",
    "        for hit in result\n",
    "    ]\n",
    "\n",
    "    documents_semantic = [fetch_info(doc) for doc in documents_semantic]\n",
    "    documents_semantic = process_documents(query, documents_semantic)\n",
    "\n",
    "    print(\"Top-3 semantic search (SBERT) hits\")\n",
    "\n",
    "    for document in documents_semantic[0:3]:\n",
    "        print('\\n###############')\n",
    "\n",
    "        if extract_answers == True:\n",
    "            answers, result = document\n",
    "            answer = answers['answer']\n",
    "            print('Answer : ', answer)\n",
    "\n",
    "        else:\n",
    "            result = document\n",
    "\n",
    "        print('\\nRetrieval Score : ', result['score'])\n",
    "        print('Wiki Title : ', result['title'])\n",
    "        print(result['content'])\n",
    "        print('\\nOriginal Document meta data : ', result['meta'])\n",
    "\n",
    "        print('#######################')\n",
    "        print()\n",
    "\n",
    "    print('SBERT Results took a total of : {} seconds.'.format(time.time()-t))\n",
    "\n",
    "    #### Re-Ranking ######\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    if combine:\n",
    "        documents_extracted = (documents_lexical + documents_semantic)\n",
    "\n",
    "        unique_hashes = set()\n",
    "        documents = []\n",
    "\n",
    "        for document in documents_extracted:\n",
    "            hash_id = document[1]['hash_id']\n",
    "\n",
    "            if hash_id not in unique_hashes:\n",
    "                unique_hashes.add(hash_id)\n",
    "                documents.append(document)\n",
    "\n",
    "    else:\n",
    "        documents = documents_semantic\n",
    "\n",
    "    cross_inp = [[query, document[1]['content']] for document in documents]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        documents[idx][1]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    documents = sorted(documents, key = lambda item: item[1].get('cross-score'), reverse = True)\n",
    "\n",
    "    print(f\"Cross Encoder Re-Ranker Scoring of {len(documents)} documents\")\n",
    "\n",
    "    for document in documents[0:3]:\n",
    "        print(\"\\n#############################\")\n",
    "\n",
    "        if extract_answers == True:\n",
    "            answers, result = document\n",
    "            answer = answers['answer']\n",
    "            print('Answer : ', answer)\n",
    "\n",
    "        else:\n",
    "            result = document\n",
    "\n",
    "        print(\"Cross Score : \", result['cross-score'])\n",
    "        print('Wiki Title : ', result['title'])\n",
    "        print(result['content'])\n",
    "        print('\\nOriginal Document meta data : ', result['meta'])\n",
    "\n",
    "        print('#######################')\n",
    "        print()\n",
    "\n",
    "    print('SBERT Results took a total of : {} seconds.'.format(time.time()-t))\n",
    "\n",
    "    return cross_scores\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_answers = True\n",
    "all_terms_must_match = False\n",
    "index = 'document_dense'\n",
    "\n",
    "query = \"Strings in python\" \n",
    "print(f\"You Searched : {query}\\n\")\n",
    "\n",
    "output = search(query, top_k = 32, index = index, model= sBERT, extract_answers=extract_answers, all_terms_must_match= all_terms_must_match, combine = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2919d230053142c8a549b601b077a4f2153c7265c40a61f9e194b26dab403fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
