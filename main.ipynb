{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyCNywAuEMOiGUkRNQa8u5UyFKPq1As8sBQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install youtube_channel_transcript_api\n",
    "%pip install --upgrade google-api-python-client\n",
    "%pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "%pip install elasticsearch\n",
    "%pip install sentence_transformers\n",
    "%pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_channel_transcript_api import *\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLIST_ID = \"PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\" \n",
    "channel_getter = YoutubePlaylistTranscripts(\"Some Gibberish Name\",PLAYLIST_ID, API_KEY) #channel getter is a YoutubePlaylistTranscripts Object\n",
    "# channel_getter is an object of 'YoutubePlaylistTranscripts' Type\n",
    "\n",
    "# for index, item in enumerate(channel_getter.video):\n",
    "#     print(f\"{index+1}. Video: {item[0]}, ID: {item[1]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching videos data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded: 21\n",
      "Number of videos data fetched: 21\n",
      "Number of videos data errored: 0\n"
     ]
    }
   ],
   "source": [
    "videos_data, videos_errored = channel_getter.get_transcripts(languages=['en'])\n",
    "\n",
    "print(f'Number of videos loaded: {len(channel_getter.video)}')\n",
    "print(f'Number of videos data fetched: {len(videos_data)}')\n",
    "print(f'Number of videos data errored: {len(videos_errored)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a backup on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_data, f)\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_errored.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_errored, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get a list of videos loaded from the playlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded from playlist: 21\n",
      "List of loaded videos:\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of loaded videos:')\n",
    "\n",
    "# for index, item in enumerate(videos_data):\n",
    "#     print(f'{index+1}. Video ID: {item}        Title:', videos_data[item]['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos loaded from playlist: 21\n",
      "List of non-loaded videos:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of non-loaded videos:')\n",
    "print(videos_errored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating caption dataset on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"./content/playlists\"\n",
    "CHANNEL_DIRECTOR_NAME = PLAYLIST_ID\n",
    "\n",
    "SAVE_FOLDER = os.path.join(ROOT_FOLDER, CHANNEL_DIRECTOR_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_data.values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid_obj in videos_data.values():\n",
    "  TITLE = vid_obj['title']\n",
    "  #windows doesn't allow all the special characters to be there in the folder name\n",
    "  # Let's remove the special characters from the title\n",
    "\n",
    "  TITLE = TITLE.replace(\"?\",'')   #windows doesn't support '?'\n",
    "  TITLE = TITLE.replace(\"|\",'')   #windows doesn't support '|'\n",
    "\n",
    "  VID_FOLDER = os.path.join(SAVE_FOLDER, TITLE)\n",
    "  # print(f'VID_FOLDER: {VID_FOLDER}')\n",
    "  vid_exists = os.path.exists(VID_FOLDER)   # checking whether the video directory exists\n",
    "  # print(f'vid_exists: {vid_exists}')\n",
    "  os.makedirs(VID_FOLDER) if not vid_exists else None   # if the directory doesn't exist, create one\n",
    "\n",
    "  vid_captions = vid_obj['captions'] \n",
    "\n",
    "  full_vid_captions = [f'Title: {TITLE}']  #This list will have all the captions in the video without the time stamps\n",
    "  #The below code can be modified to include time\n",
    "  for caption in vid_captions:\n",
    "    full_vid_captions.append(caption['text'])   #full video captions is the list of caption strings\n",
    "\n",
    "  full_vid_captions = \" \".join(full_vid_captions)   # this returns a single string of complete video caption\n",
    "\n",
    "  with open(os.path.join(VID_FOLDER, f'{TITLE}_captions.txt'), 'w') as f:\n",
    "    f.write(full_vid_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_document(text:str) -> str:\n",
    "    # this function tries to clean the text by removing multiple new lines, adding paragraph breaks, and removing empty paragraphs\n",
    "\n",
    "    # getting rid of all new lines\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', '')\n",
    "\n",
    "    # will add some features here in future\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Document Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import hashlib\n",
    "import mmh3\n",
    "from typing import List, Dict\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, meta, hash_id, title:str, content:str, language:str = 'English', score:float = None, hash_id_keys:List[str] = None):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.language = language\n",
    "        self.hash_id_keys = hash_id_keys\n",
    "        self.meta = meta\n",
    "\n",
    "        if hash_id is None: \n",
    "            self.hash_id = self.generate_hash(hash_id_keys)\n",
    "        else:\n",
    "            self.hash_id = hash_id\n",
    "\n",
    "    def generate_hash(self, hash_id_keys):\n",
    "        return \"{:02x}\".format(mmh3.hash128(str(self.content), signed=False))\n",
    "\n",
    "    def to_dict(self, field_map = {}):\n",
    "        inv_field_map = {v:k for k, v in field_map.items()}\n",
    "        _doc: Dict[str, str] = {}\n",
    "\n",
    "        for k, v in self.__dict__.items():\n",
    "            # Exclude other fields (Pydantic, ..) fields from the conversion process\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            k = k if k not in inv_field_map else inv_field_map[k]\n",
    "        return _doc\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Title: {self.title}\\nContent: {self.content}\\nLanguage: {self.language}\\nHash ID: {self.hash_id} \\nMetadata: {self.meta}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(document:Document, split_length:int = 100):\n",
    "    text = document.content\n",
    "\n",
    "    line = ''\n",
    "    text_chunks = []\n",
    "\n",
    "    words = text.split(' ')[:-1]\n",
    "\n",
    "    # print(words)\n",
    "\n",
    "    for word in words:\n",
    "        if len(line) >= split_length:\n",
    "            text_chunks.append(line)\n",
    "            line = ''\n",
    "\n",
    "        else:\n",
    "            line += ' ' + word\n",
    "            \n",
    "    # for sentence in (s.strip() + '.' for s in text.split('.')[:-1]):   \n",
    "    #     if len(line.split()) + len(sentence.split()) + 1 >= split_length:   # can't fit on that line => start a new one\n",
    "    #         text_chunks.append(line)\n",
    "    #         line = sentence\n",
    "            \n",
    "    #     else:       # can fit it => add a space and then the sentence\n",
    "    #         line += '' + sentence\n",
    "\n",
    "    # print(f'text chnks are: {text_chunks}')\n",
    "\n",
    "    documents = []\n",
    "    for i, txt in enumerate(text_chunks):\n",
    "        doc = Document(title = document.title, content = txt, hash_id = None, hash_id_keys=None, meta = {'filename': document.meta.copy()} or {})\n",
    "        # I need to implement meta data here\n",
    "        doc.meta[\"_split_id\"] = i\n",
    "        doc.meta[\"_parent_hash\"] = document.hash_id\n",
    "        documents.append(doc)\n",
    "        \n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: Control Flow in Python - If Elif Else Statements In almost every program, there are timesyou need to make decisions and that's when you use an if statement here is anexample let's say we have a variable called temperature we set it to 35 nowif temperature is greater than 30 perhaps we want to display a message tothe user so we use an if statement if after if we add a condition which isbasically a boolean expression an expression that produces a boolean valueso if temperature is greater than 30 here we have a boolean expression ifthis expression evaluates to true the following statements will be executedlet me show you now here is the important part that a lot of beginnersmiss when you use an if statement you should always terminate yourstatement with a colon now let's see what happens when I press ENTER ourcursor is indented so here we have two white spaces this is very importantbecause using these indentations Python interpreter will know what statementsshould be executed if this\n",
      " is true here we want to print a messagelike it's warm we can print another message as well drink water so we canhave as many statements as we want here as long as they are indented they belongto this if block now when we finish here we should remove indentation to indicatethe end of this if block so here we can add a print statement with a messagelike done this statement will always be executed whether this condition is trueor not now note that when I save the changes this indentation you see here isgoing to be doubled up take a look save there you go so when we save thechanges auto kept eight reformat our code and uses four white spaces forindentation so one two three four it uses four white spaces because that'swhat pepp eight recommends alright now let's run this program so becausetemperature is greater than 30 we see the first twomessages and we see the dawn message regardless so if I change thetemperature to let's say 15 and run the program one more timelook this dawn message is executed\n",
      " our condition is true or not sopay great attention to these indentations that's one of the issues Isee in beginners code let's say they want both these print statements to beexecuted if the condition is true accidentally they remove the indentationon the fourth line and that's why their program doesn't work as they expect sobe careful about this now what if you want to have multiple conditions we usean elephant so L if that is short for else/if here we can add anothercondition another expression so temperature is greater than 20 onceagain : enter a by default here vs code is usingtwo white spaces so don't worry about this as soon as you save the changesthose two white spaces will be converted to four white spaces so let's print adifferent message it's nice save the changes now look allthese lines are indented consistently you can have as many Elif statements asyou want and optionally you can also have an else statement so if none of theprevious conditions are true then what you have in\n",
      " else block will beexecuted once again we add the colon annotation print here we can add amessage like it's called saying the changes in this case temperature is 15so none of these two conditions will be true and we will see it's called let'srun the program there you go in this lecture I'm going to show you atechnique for writing cleaner code so let's say we're building an applicationfor university and we want to check to see if the person who's applying forthis university program is eligible or not so we start by defining a variablecalled age set it to 22 now if H is greater than or equal to 18 colon printeligible remove the indentation else colon print not eligible let's run aprogram make sure it works beautiful now there is nothing wrong in this piece ofcode but I want to show you a cleaner way to achieve the same result insteadof having a print statement here we can define a variable like message and setit to this string that is the first step so message equals this string and thenwe\n",
      " print this message now when you have an if-else statement with thisstructure where you're basically assigning a value to a variable you canrewrite this in a simpler way so this is how it works all we want to do overthese few lines is to assign a value to this message variable right so we startwith message we set it to eligible if H is greater than or equal to 18 else weset it to not eligible this statement is almost like plainEnglish so what we have on line 7 is exactly equivalent to these 4 lines ofcode delete save the changes run the programyou can see this person is eligible if I change the age to 12 and run the programwe get not eligible so what we have here is called ternaryoperator in Python we have three logicaloperators and we use these operators to model more complex conditions so theseoperators are and or and not let's see a real-world example of using theseoperators so imagine we're building an application for processing loans so weneed two variables high income we can set this\n",
      " true and good underlinecredit we set it to true now here's the condition we want to implement if theapplicant has high income and good credit score then they are eligible forthe loan so if high income and good credit we addthe colon and print eligible now note that here I have not compared the valueof this variable with true that is one of the issues I see in a lot ofbeginners code this is redundant and unprofessional because high income is aboolean so it's either true or false we don't need to compare true with true soif this condition is true and this second condition is true then we willprint eligible in the terminal so save the changes and run the programobviously this person is eligible however if one of these conditions isfalse we will not see eligible in the terminal so let's add an else statementhere and print not eligible run the program we see not eligible so this ishow the and operator works with an operator if both conditions are true theresult will be true in contrast with\n",
      " or operator as long as at least one ofthe conditions is true the result will be true so if I replace and with or herewe should see eligible and the terminal let's run it one more time there you goso these are the and an or operator now let's take a look at an example ofthe nut operator so I'm gonna define another variable steel and set it totrue temporarily I'm gonna remove this expression and simplify it we'll comeback to this later so let's say if the person is eligible if they are not astudent the nut operator basically inverses thevalue of a boolean so in this case student is true when we apply the notoperator the result will be false so in this case our condition will be falseand that's why this print statement will not be executed let me show you so saverun the program they are not eligible if student was false when we apply the nutoperator will get true so our condition will be true and we'll see it eligiblelet's run it one more time there you go with this operators we can model\n",
      " complex conditions here is an example a person can be eligible if theyhave either high income or good credit and they should not be used to do thatlet me show you how to implement this condition so if high income or goodcredit we want at least one of these conditions to be true so we put these inparentheses we want to separate these from the other condition which is not astudent now the result of this should be true which means at least one of theseconditions should be true after that we'll add and not studentand finally caught so we this operators you can model all kinds of real-worldscenarios so here's the example from the lastlecture a person is eligible for a loan if they have high income and good creditand they are not a student now one thing you need to know aboutthese boolean operators is that their short circuit what do we mean by thatwell when python interpreter wants to evaluate this expression itstarts from the first argument if this is true it continues the evaluation tosee\n",
      " the second argument is also true so it continues the evaluation all theway to the end of this expression however as soon as one of thesearguments is false the evaluation stops let me show you what I mean so if Ichange high-income to false when Python interpreter sees this expression itstarts here it knows that high-income is false so it doesn't matter what comesafter the result of this entire expression will always be false becauseat least one of the arguments or one of the operands is false this is what wecall short-circuiting just like the short-circuit concept we have inelectronics so the evaluation stops as soon as one of these arguments evaluatesto false we have the same concept with the or operator so if I change these andoperators to or let's see what happens with the or operator we know that atleast one of the arguments should be true so the evaluation stops as soon aswe find an argument that evaluates to true in this case when Pythoninterpreter evaluates this expression it sees\n",
      " high-income is false so itcontinues the evaluation hoping that the next argument will be true here goodcredit is true so evaluation stops and the result of this entire expressionwill be true so in Python logical operators areshort-circuit in this lecture I'm gonna show you howto chain comparison operators this is a very powerful technique for writingclean code here is an example let's say we want to implement a rule that saysage should be between 18 and 65 here's how we can implement it so we define avariable like age set it to 22 now if age is greater than or equal to 18 andage is less than 65 then we print eligible now here's a question for youhow do we write this rule in mass we can write it like this well more accurately we should have anequal sign here so age should be between 18 and 65 this is how we write this rulein math now I've got some good news for you we can write the exact sameexpression in Python so I'm going to move this up put an ifstatement here line 4 and line 3 are\n",
      " equivalent but as you can seeline 4 is cleaner and easier to read so let's get rid of line 3this is what we call chaining comparison operators all right here is a little quiz for youI want you to pause the video and think about this quiz for 10 to 20 secondswhat do you think we'll see on the terminal when we run this program sopause the video figure out the answer when you're ready come back continuewatching all right let's see what happens when werun this program first we get this if statement in this case we're comparingtwo different objects for equality and these objects have different types wehave a number compared for this string so number 10 and string 10 are not equalthat is why a will not be printed on the terminal so the control moves to theElif part here we have two boolean expressions here's the first one here'sthe second one and they are combined using the logical end so if both theseexpressions are evaluated to true then this entire expression will be true andwe will see beyond\n"
     ]
    }
   ],
   "source": [
    "f = open('./content/playlists\\PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\\Control Flow in Python - If Elif Else Statements\\Control Flow in Python - If Elif Else Statements_captions.txt', 'r')\n",
    "content = f.read()\n",
    "obj = Document(title = 'hi', content = content, meta = {'file_name': f'Control Flow in Python - If Elif Else Statements_captions.txt'} , hash_id = None, hash_id_keys = None)\n",
    "obj.content = clean_document(obj.content)\n",
    "docs = split_documents(obj,  split_length = 1000)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:The file ./content/playlists\\PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\\The 3 MOST IMPORTANT JAZZ SCALES and how similar they are\\The 3 MOST IMPORTANT JAZZ SCALES and how similar they are_captions.txt cannot be opened.\n"
     ]
    }
   ],
   "source": [
    "next_folder = os.path.join(SAVE_FOLDER, os.listdir(SAVE_FOLDER)[0])\n",
    "file_path = os.path.join(next_folder, f'{os.listdir(SAVE_FOLDER)[0]}_captions.txt')\n",
    "\n",
    "parent_document = {}    # storing document objects with the hashid:object \n",
    "document_list = []      # this list stores all the document objects\n",
    "split = True\n",
    "\n",
    "# crawler\n",
    "for folder in os.listdir(SAVE_FOLDER):\n",
    "    # opening the files\n",
    "    next_folder = os.path.join(SAVE_FOLDER, folder)\n",
    "    file_path = os.path.join(next_folder, f'{folder}_captions.txt')\n",
    "    \n",
    "\n",
    "    try:\n",
    "        f = open(file_path, 'r')\n",
    "    except:\n",
    "        logging.error(f\"The file {file_path} cannot be opened.\")\n",
    "    \n",
    "    # creating document object \n",
    "    content = f.read()\n",
    "    obj = Document(title = folder, content = content, meta = {'file_name': f'{folder}_captions.txt'} , hash_id = None, hash_id_keys = None)\n",
    "\n",
    "    # cleaning the object content\n",
    "    obj.content = clean_document(obj.content)\n",
    "\n",
    "    # storing the content in the dictionary\n",
    "    parent_document[obj.hash_id] = obj\n",
    "\n",
    "\n",
    "    # if split is needed, we split else we directly append to the list\n",
    "    if split:\n",
    "        # split_document returns a list of document objects\n",
    "        documents = split_documents(obj, split_length = 1000)\n",
    "\n",
    "\n",
    "        # appending the list of document objects to our main list\n",
    "        for d in documents:\n",
    "            document_list.append(d)\n",
    "        \n",
    "    else:\n",
    "        document_list.append(obj)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: Control Flow in Python - If Elif Else Statements In almost every program, there are timesyou need to make decisions and that's when you use an if statement here is anexample let's say we have a variable called temperature we set it to 35 nowif temperature is greater than 30 perhaps we want to display a message tothe user so we use an if statement if after if we add a condition which isbasically a boolean expression an expression that produces a boolean valueso if temperature is greater than 30 here we have a boolean expression ifthis expression evaluates to true the following statements will be executedlet me show you now here is the important part that a lot of beginnersmiss when you use an if statement you should always terminate yourstatement with a colon now let's see what happens when I press ENTER ourcursor is indented so here we have two white spaces this is very importantbecause using these indentations Python interpreter will know what statementsshould be executed if this\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_list[0].content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ES!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"D:\\BTP\\youtubeQandA\\http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", 'vWr8xqxdlmOhj*Q_2yFI')\n",
    ")\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to ES!\")\n",
    "else:\n",
    "    print(\"Could not connect!\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "\n",
    "def bulk_index_documents(documents_to_index, request_timeout = 300, refresh = 'wait_for'):\n",
    "    try:\n",
    "        bulk(es, documents_to_index, request_timeout = request_timeout, refresh = refresh)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to index batch of {len(documents_to_index)} documents because of too many request response\")\n",
    "\n",
    "\n",
    "def create_mappings_document(\n",
    "    index_name = 'document',\n",
    "    analyzer = 'standard',\n",
    "    custom_mapping = None,\n",
    "    name_field = \"title\",\n",
    "    content_field = \"content\",\n",
    "    embedding_field = 'embedding',\n",
    "    embedding_dim = 768,\n",
    "    search_fields = ['content'],\n",
    "    synonyms = None,\n",
    "    synonym_type = 'synonym'):\n",
    "    \n",
    "\n",
    "    if custom_mapping:\n",
    "        mapping = custom_mapping\n",
    "    else:\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {name_field : {\"type\" : \"keyword\"}, content_field: {\"type\": \"text\"}},\n",
    "                \"dynamic_templates\": [\n",
    "                    {\"strings\": {\"path_match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": {\"type\": \"keyword\"}}}\n",
    "                ],\n",
    "            },\n",
    "            \"settings\": {\"analysis\": {\"analyzer\": {\"default\": {\"type\": analyzer}}}},\n",
    "        }\n",
    "\n",
    "    if synonyms:\n",
    "        for field in search_fields:\n",
    "            mapping[\"mappings\"][\"properties\"].update({field: {\"type\": \"text\", \"analyzer\": \"synonym\"}})\n",
    "        mapping[\"mappings\"][\"properties\"][content_field] = {\"type\": \"text\", \"analyzer\": \"synonym\"}\n",
    "\n",
    "        mapping[\"settings\"][\"analysis\"][\"analyzer\"][\"synonym\"] = {\n",
    "            \"tokenizer\": \"whitespace\",\n",
    "            \"filter\": [\"lowercase\", \"synonym\"],\n",
    "        }\n",
    "\n",
    "        mapping[\"settings\"][\"analysis\"][\"filter\"] = {\n",
    "            \"synonym\": {\"type\": synonym_type, \"synonyms\": synonyms}\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        for field in search_fields:\n",
    "            mapping[\"mappings\"][\"properties\"].update({field: {\"type\": \"text\"}})\n",
    "\n",
    "    if embedding_field:\n",
    "        mapping[\"mappings\"][\"properties\"][embedding_field] = {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": embedding_dim\n",
    "        }\n",
    "\n",
    "    es.indices.create(index = index_name, ignore = 400, body = mapping)\n",
    "\n",
    "\n",
    "def index_documents(documents, index = 'document', batch_size = 100, refresh_type = 'wait_for'):\n",
    "    if index and not es.indices.exists(index= index):\n",
    "        logging.info('Creating mappings for the index as user did not provide any custom mapping...')\n",
    "        create_mappings_document(index_name = index)\n",
    "\n",
    "    else:\n",
    "        logging.info('Using custom mapping...')\n",
    "\n",
    "    documents_to_index = []\n",
    "\n",
    "    # Iterating through all the documents and indexing them together\n",
    "    for i, doc in enumerate(documents):\n",
    "        \n",
    "        # First we convert the document object into dict to follow ES conventions\n",
    "        _doc = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index,\n",
    "            **doc.to_dict()\n",
    "        }\n",
    "\n",
    "        _doc[\"_id\"] = str(i)\n",
    "\n",
    "        # Cast the embedding type as ES does not support numpy\n",
    "        if _doc['embedding'] is not None:\n",
    "            if (type(_doc['embedding']) == np.ndarray):\n",
    "                _doc['embedding'] = _doc['embedding'].tolist()\n",
    "\n",
    "        # don't index query score and empty fields\n",
    "        _ = _doc.pop(\"score\", None)\n",
    "        _doc = {k: v for k, v in _doc.items() if v is not None}\n",
    "\n",
    "        # For flat structure generally used in Elastic Search\n",
    "        # we 'unnest' all value within \"meta\"\n",
    "        if \"meta\" in _doc.keys():\n",
    "            for k, v in _doc[\"meta\"].items():\n",
    "                _doc[k] = v\n",
    "            _doc.pop(\"meta\")\n",
    "\n",
    "        documents_to_index.append(_doc)\n",
    "\n",
    "        if len(documents_to_index) % batch_size == 0:\n",
    "            logging.info(f'Indexing {len(documents_to_index)} documents')\n",
    "            bulk_index_documents(documents_to_index, request_timeout=300, refresh = refresh_type)\n",
    "            documents_to_index = []\n",
    "\n",
    "        if documents_to_index:\n",
    "            logging.info(f'Indexing {len(documents_to_index)} documents')\n",
    "            bulk_index_documents(documents_to_index, request_timeout=300, refresh = refresh_type)\n",
    "\n",
    "def clear_indices(index_name = ''):\n",
    "    \n",
    "    if index_name == '':\n",
    "        indices = list(es.indices.get_alias(\"*\").keys())\n",
    "    else:\n",
    "        indices = list(es.indices.get_alias(index_name).keys())\n",
    "\n",
    "    if indices:\n",
    "        logging.info(f'Wiping out all the documents belonging to the following indices: {indices}')\n",
    "\n",
    "        es.delete_by_query(index = indices, body = {\"query\": {\"match_all\": {}}})\n",
    "\n",
    "        logging.info(f\"Deleting Indices\")\n",
    "        for index in indices:\n",
    "            es.indices.delete(index= index, ignore= [400, 404])\n",
    "\n",
    "        print(list(es.indices.get_alias(\"*\".keys())))\n",
    "\n",
    "    else:\n",
    "        logging.info(\"No Indices are present in storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the query for the default BM25 retriever\n",
    "\n",
    "def construct_query(query, top_k, filters = None, all_terms_must_match = False):\n",
    "\n",
    "    # We choose if all terms must match or not (this can be used to enforce more strict rules)\n",
    "    operator = \"AND\" if all_terms_must_match else \"OR\"\n",
    "\n",
    "# There are multiple options for keyword based serach such as:\n",
    "# -> match: It directly matches all the keywords in any order\n",
    "# -> match_phrase: It directly matches all the keywords in specific order( so that sentences are bound to make sense)\n",
    "# -> multi_match: It directly matches all the keywords in any order but can match on multiple fields\n",
    "\n",
    "    body = {\n",
    "        \"size\": str(top_k),\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"content\", \"title\"],\n",
    "                            \"operator\": operator,\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "# This constructs query for dense retrieval using various scores\n",
    "def construct_query_dense(query_vector, top_k, similarity = \"cosinse\"):\n",
    "\n",
    "    if similarity == \"cosine\":\n",
    "        similarity_fn_name = \"cosineSimilarity\"\n",
    "    elif similarity == \"dot_product\":\n",
    "        similarity_fn_name = \"dotProduct\"\n",
    "    elif similarity == \"12\":\n",
    "        similarity_fn_name = \"12norm\"\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Invalid value for similarity in ElasticSearchDocumentStore\\nChoose between 'cosine', 'dot_product', and '12\"\n",
    "        )\n",
    "    \n",
    "    logging.info(f'Using the following similarity metric : {similarity}')\n",
    "\n",
    "    if (type(query_vector) == np.ndarray):\n",
    "        query_vector = query_vector.tolist()\n",
    "    \n",
    "    logging.info(f'The type of query vector is: {type(query_vector)}')\n",
    "\n",
    "    body = {\n",
    "        \"size\": str(top_k),\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": f\"{similarity_fn_name}(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vector}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "# This function processes the hit results obtained after elastic search\n",
    "def convert_es_dict(es_dict, return_embedding = False, scale_score = None):\n",
    "\n",
    "    meta_data = {k : v for k,v in es_dict['_source'].item() if k not in ('title', 'content', 'language', 'hash_id', 'embedding')}\n",
    "\n",
    "    # calculate score if using embedding retreival\n",
    "    score = es_dict['_score']\n",
    "\n",
    "    # check if name field is present or not\n",
    "    if es_dict['_source']['title'] is not None:\n",
    "        title = es_dict['_source']['title']\n",
    "\n",
    "    document = Document(title = title, content = es_dict['_source']['content'], language = es_dict['_source']['language'], meta = meta_data, score = score, hash_id = es_dict['_source']['hash_id'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dense Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/tutorial'\n",
    "documents_dense, parent_dense = document_list, parent_document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# good for passage search\n",
    "sBERT = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBERT.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1936267658.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[81], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    temp =\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# temp = []\n",
    "# for i in range(5):\n",
    "#     temp.append(documents_dense[i].content)\n",
    "\n",
    "# encoded_data = sBERT.encode([temp])\n",
    "\n",
    "temp =\n",
    "for doc in documents_dense:\n",
    "    print([doc.content for doc in documents_dense])\n",
    "# encoded_data = sBERT.encode([doc.content for doc in documents_dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(documents_dense):\n\u001b[1;32m----> 2\u001b[0m     doc\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m encoded_data[i]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents_dense):\n",
    "    doc.embedding = encoded_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print((documents_dense[0].embedding).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute '_Document__dict__items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index_documents(documents_dense, index \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdocument_dense\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[46], line 82\u001b[0m, in \u001b[0;36mindex_documents\u001b[1;34m(documents, index, batch_size, refresh_type)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m# Iterating through all the documents and indexing them together\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(documents):\n\u001b[0;32m     77\u001b[0m     \n\u001b[0;32m     78\u001b[0m     \u001b[39m# First we convert the document object into dict to follow ES conventions\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     _doc \u001b[39m=\u001b[39m {\n\u001b[0;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_op_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_index\u001b[39m\u001b[39m\"\u001b[39m: index,\n\u001b[1;32m---> 82\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdoc\u001b[39m.\u001b[39;49mto_dict()\n\u001b[0;32m     83\u001b[0m     }\n\u001b[0;32m     85\u001b[0m     _doc[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(i)\n\u001b[0;32m     87\u001b[0m     \u001b[39m# Cast the embedding type as ES does not support numpy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 21\u001b[0m, in \u001b[0;36mDocument.to_dict\u001b[1;34m(self, field_map)\u001b[0m\n\u001b[0;32m     18\u001b[0m inv_field_map \u001b[39m=\u001b[39m {v:k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m field_map\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m _doc: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__dict__items():\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Exclude other fields (Pydantic, ..) fields from the conversion process\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     24\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Document' object has no attribute '_Document__dict__items'"
     ]
    }
   ],
   "source": [
    "index_documents(documents_dense, index = 'document_dense', batch_size = 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to define a reader for better results\n",
    "\n",
    "from transformers import BertForQuestionAnswering, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2919d230053142c8a549b601b077a4f2153c7265c40a61f9e194b26dab403fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
