{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyCNywAuEMOiGUkRNQa8u5UyFKPq1As8sBQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install youtube_channel_transcript_api\n",
    "%pip install --upgrade google-api-python-client\n",
    "%pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "%pip install elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_channel_transcript_api import *\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLIST_ID = \"PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\" \n",
    "channel_getter = YoutubePlaylistTranscripts(\"Some Gibberish Name\",PLAYLIST_ID, API_KEY) #channel getter is a YoutubePlaylistTranscripts Object\n",
    "# channel_getter is an object of 'YoutubePlaylistTranscripts' Type\n",
    "\n",
    "# for index, item in enumerate(channel_getter.video):\n",
    "#     print(f\"{index+1}. Video: {item[0]}, ID: {item[1]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching videos data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_data, videos_errored = channel_getter.get_transcripts(languages=['en'])\n",
    "\n",
    "print(f'Number of videos loaded: {len(channel_getter.video)}')\n",
    "print(f'Number of videos data fetched: {len(videos_data)}')\n",
    "print(f'Number of videos data errored: {len(videos_errored)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a backup on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_data, f)\n",
    "with open(f'./content/{PLAYLIST_ID}_vids_data_errored.pkl', 'wb') as f:\n",
    "    pickle.dump(videos_errored, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get a list of videos loaded from the playlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of loaded videos:')\n",
    "\n",
    "# for index, item in enumerate(videos_data):\n",
    "#     print(f'{index+1}. Video ID: {item}        Title:', videos_data[item]['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of videos loaded from playlist: {len(videos_data)}')\n",
    "print('List of non-loaded videos:')\n",
    "print(videos_errored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating caption dataset on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"./content/playlists\"\n",
    "CHANNEL_DIRECTOR_NAME = PLAYLIST_ID\n",
    "\n",
    "SAVE_FOLDER = os.path.join(ROOT_FOLDER, CHANNEL_DIRECTOR_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_data.values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid_obj in videos_data.values():\n",
    "  TITLE = vid_obj['title']\n",
    "  #windows doesn't allow all the special characters to be there in the folder name\n",
    "  # Let's remove the special characters from the title\n",
    "\n",
    "  TITLE = TITLE.replace(\"?\",'')   #windows doesn't support '?'\n",
    "  TITLE = TITLE.replace(\"|\",'')   #windows doesn't support '|'\n",
    "\n",
    "  VID_FOLDER = os.path.join(SAVE_FOLDER, TITLE)\n",
    "  # print(f'VID_FOLDER: {VID_FOLDER}')\n",
    "  vid_exists = os.path.exists(VID_FOLDER)   # checking whether the video directory exists\n",
    "  # print(f'vid_exists: {vid_exists}')\n",
    "  os.makedirs(VID_FOLDER) if not vid_exists else None   # if the directory doesn't exist, create one\n",
    "\n",
    "  vid_captions = vid_obj['captions'] \n",
    "\n",
    "  full_vid_captions = [f'Title: {TITLE}']  #This list will have all the captions in the video without the time stamps\n",
    "  #The below code can be modified to include time\n",
    "  for caption in vid_captions:\n",
    "    full_vid_captions.append(caption['text'])   #full video captions is the list of caption strings\n",
    "\n",
    "  full_vid_captions = \" \".join(full_vid_captions)   # this returns a single string of complete video caption\n",
    "\n",
    "  with open(os.path.join(VID_FOLDER, f'{TITLE}_captions.txt'), 'w') as f:\n",
    "    f.write(full_vid_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_document(text:str) -> str:\n",
    "    # this function tries to clean the text by removing multiple new lines, adding paragraph breaks, and removing empty paragraphs\n",
    "\n",
    "    # getting rid of all new lines\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', '')\n",
    "\n",
    "    # will add some features here in future\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Document Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import hashlib\n",
    "import mmh3\n",
    "from typing import List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, meta, hash_id, title:str, content:str, language:str = 'English', score:float = None, hash_id_keys:List[str] = None):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.language = language\n",
    "        self.hash_id_keys = hash_id_keys\n",
    "        self.meta = meta\n",
    "\n",
    "        if hash_id is None: \n",
    "            self.hash_id = self.generate_hash(hash_id_keys)\n",
    "        else:\n",
    "            self.hash_id = hash_id\n",
    "\n",
    "    def generate_hash(self, hash_id_keys):\n",
    "        return \"{:02x}\".format(mmh3.hash128(str(self.content), signed=False))\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Title: {self.title}\\nContent: {self.content}\\nLanguage: {self.language}\\nHash ID: {self.hash_id} \\nMetadata: {self.meta}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(document:Document, split_length:int = 100):\n",
    "    text = document.content\n",
    "\n",
    "    line = ''\n",
    "    text_chunks = []\n",
    "\n",
    "    words = text.split(' ')[:-1]\n",
    "\n",
    "    # print(words)\n",
    "\n",
    "    for word in words:\n",
    "        if len(line) >= split_length:\n",
    "            text_chunks.append(line)\n",
    "            line = ''\n",
    "\n",
    "        else:\n",
    "            line += ' ' + word\n",
    "            \n",
    "    # for sentence in (s.strip() + '.' for s in text.split('.')[:-1]):   \n",
    "    #     if len(line.split()) + len(sentence.split()) + 1 >= split_length:   # can't fit on that line => start a new one\n",
    "    #         text_chunks.append(line)\n",
    "    #         line = sentence\n",
    "            \n",
    "    #     else:       # can fit it => add a space and then the sentence\n",
    "    #         line += '' + sentence\n",
    "\n",
    "    # print(f'text chnks are: {text_chunks}')\n",
    "\n",
    "    documents = []\n",
    "    for i, txt in enumerate(text_chunks):\n",
    "        doc = Document(title = document.title, content = txt, hash_id = None, hash_id_keys=None, meta = {'filename': document.meta.copy()} or {})\n",
    "        # I need to implement meta data here\n",
    "        doc.meta[\"_split_id\"] = i\n",
    "        doc.meta[\"_parent_hash\"] = document.hash_id\n",
    "        documents.append(doc)\n",
    "        \n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./content/playlists\\PLTjRvDozrdlxj5wgH4qkvwSOdHLOCx10f\\Control Flow in Python - If Elif Else Statements\\Control Flow in Python - If Elif Else Statements_captions.txt', 'r')\n",
    "content = f.read()\n",
    "obj = Document(title = 'hi', content = content, meta = {'file_name': f'Control Flow in Python - If Elif Else Statements_captions.txt'} , hash_id = None, hash_id_keys = None)\n",
    "obj.content = clean_document(obj.content)\n",
    "docs = split_documents(obj,  split_length = 1000)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_folder = os.path.join(SAVE_FOLDER, os.listdir(SAVE_FOLDER)[0])\n",
    "file_path = os.path.join(next_folder, f'{os.listdir(SAVE_FOLDER)[0]}_captions.txt')\n",
    "\n",
    "parent_document = {}    # storing document objects with the hashid:object \n",
    "document_list = []      # this list stores all the document objects\n",
    "split = True\n",
    "\n",
    "# crawler\n",
    "for folder in os.listdir(SAVE_FOLDER):\n",
    "    # opening the files\n",
    "    next_folder = os.path.join(SAVE_FOLDER, folder)\n",
    "    file_path = os.path.join(next_folder, f'{folder}_captions.txt')\n",
    "    \n",
    "\n",
    "    try:\n",
    "        f = open(file_path, 'r')\n",
    "    except:\n",
    "        logging.error(f\"The file {file_path} cannot be opened.\")\n",
    "    \n",
    "    # creating document object \n",
    "    content = f.read()\n",
    "    obj = Document(title = folder, content = content, meta = {'file_name': f'{folder}_captions.txt'} , hash_id = None, hash_id_keys = None)\n",
    "\n",
    "    # cleaning the object content\n",
    "    obj.content = clean_document(obj.content)\n",
    "\n",
    "    # storing the content in the dictionary\n",
    "    parent_document[obj.hash_id] = obj\n",
    "\n",
    "\n",
    "    # if split is needed, we split else we directly append to the list\n",
    "    if split:\n",
    "        # split_document returns a list of document objects\n",
    "        documents = split_documents(obj, split_length = 1000)\n",
    "\n",
    "\n",
    "        # appending the list of document objects to our main list\n",
    "        for d in documents:\n",
    "            document_list.append(d)\n",
    "        \n",
    "    else:\n",
    "        document_list.append(obj)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_transport.py\", line 329, in perform_request\n",
      "    meta, raw_data = node.perform_request(\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_node\\_http_urllib3.py\", line 199, in perform_request\n",
      "    raise err from None\n",
      "elastic_transport.ConnectionError: Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 2 times in a row, putting on 2 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 1 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_transport.py\", line 329, in perform_request\n",
      "    meta, raw_data = node.perform_request(\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_node\\_http_urllib3.py\", line 199, in perform_request\n",
      "    raise err from None\n",
      "elastic_transport.ConnectionError: Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 3 times in a row, putting on 4 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 2 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_transport.py\", line 329, in perform_request\n",
      "    meta, raw_data = node.perform_request(\n",
      "  File \"c:\\Users\\Chambal Ka Daku\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elastic_transport\\_node\\_http_urllib3.py\", line 199, in perform_request\n",
      "    raise err from None\n",
      "elastic_transport.ConnectionError: Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 4 times in a row, putting on 8 second timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to ES!\")\n",
    "else:\n",
    "    print(\"Could not connect!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ES!\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"D:\\BTP\\youtubeQandA\\http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", 'vWr8xqxdlmOhj*Q_2yFI')\n",
    ")\n",
    "\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to ES!\")\n",
    "else:\n",
    "    print(\"Could not connect!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2919d230053142c8a549b601b077a4f2153c7265c40a61f9e194b26dab403fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
